{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pungen.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mleshen/pungen/blob/master/pungen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YchTxY0I5S_-",
        "colab_type": "text"
      },
      "source": [
        "# Untar BookCorpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_61S7iAkilQl",
        "colab_type": "code",
        "outputId": "532ffc2d-80c3-4586-9708-314374edc69f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svljy4wEjDLB",
        "colab_type": "code",
        "outputId": "3593a1bc-37dd-4f23-f29a-a98139a77417",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!tar -xvf /content/drive/My\\ Drive/Thesis/bookcorpus.tar.bz2\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "books_large_p2.txt\n",
            "books_large_p1.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTnNntJln98E",
        "colab_type": "code",
        "outputId": "7f3bb827-a7ef-492e-cb07-bc046792dc1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "!head -20 books_large_p2.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usually , he would be tearing around the living room , playing with his toys .\n",
            "but just one look at a minion sent him practically catatonic .\n",
            "that had been megan 's plan when she got him dressed earlier .\n",
            "he 'd seen the movie almost by mistake , considering he was a little young for the pg cartoon , but with older cousins , along with her brothers , mason was often exposed to things that were older .\n",
            "she liked to think being surrounded by adults and older kids was one reason why he was a such a good talker for his age .\n",
            "`` are n't you being a good boy ? ''\n",
            "she said .\n",
            "mason barely acknowledged her .\n",
            "instead , his baby blues remained focused on the television .\n",
            "since the movie was almost over , megan knew she better slip into the bedroom and finish getting ready .\n",
            "each time she looked into mason 's face , she was grateful that he looked nothing like his father .\n",
            "his platinum blond hair and blue eyes were completely hers .\n",
            "it was only his build that he was taking after his father .\n",
            "where megan was a diminutive 5'3 '' , davis was 6'1 '' and two hundred pounds .\n",
            "mason was already registering off the charts in height and weight according to his pediatrician .\n",
            "davis had seen mason only twice in his lifetime-the day he had been born and the day he came home from the hospital .\n",
            "after that , he had n't been interested in any of the pictures and emails megan sent .\n",
            "with his professional football career on the rise , davis had n't wanted to be shackled with the responsibilities of a baby .\n",
            "instead , he wanted to spend his time off the field partying until all hours of the night .\n",
            "he only paid child support when megan threatened to have his wages garnished .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kY6XTUgoltL",
        "colab_type": "code",
        "outputId": "f090f7e3-8027-4735-f9ea-9292c4cde356",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls -sh books_large_p2.txt"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0G books_large_p2.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4j7jkz8IotiN",
        "colab_type": "code",
        "outputId": "cfe32f6f-d30d-463c-9a3b-6cac75eb9be7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls -sh books_large_p1.txt"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4G books_large_p1.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylw3YX6_DCtJ",
        "colab_type": "text"
      },
      "source": [
        "# Preprocess raw text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O61Dej5RDtgu",
        "colab_type": "code",
        "outputId": "b8c63249-2998-423d-8d33-ad4b2863d41f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "!pip install unidecode"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\r\u001b[K     |█▍                              | 10kB 27.2MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20kB 3.1MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 61kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 71kB 4.3MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 92kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 102kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 112kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 122kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 133kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 143kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 153kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 163kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 174kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 184kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 194kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 204kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 215kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 225kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 235kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 245kB 4.8MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5F7MWQg6Yq8M",
        "colab_type": "code",
        "outputId": "a2943d87-517f-46d1-9c8b-16e5ff7ab660",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dV1gAOwzZFN5",
        "colab_type": "code",
        "outputId": "1890f306-dba1-40ea-f045-b00ee30afc11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "!python -m spacy validate"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r⠙ Loading compatibility table...\r\u001b[2K\u001b[38;5;2m✔ Loaded compatibility table\u001b[0m\n",
            "\u001b[1m\n",
            "====================== Installed models (spaCy v2.1.9) ======================\u001b[0m\n",
            "\u001b[38;5;4mℹ spaCy installation: /usr/local/lib/python3.6/dist-packages/spacy\u001b[0m\n",
            "\n",
            "TYPE      NAME             MODEL            VERSION                            \n",
            "package   en-core-web-sm   en_core_web_sm   \u001b[38;5;2m2.1.0\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
            "link      en               en_core_web_sm   \u001b[38;5;2m2.1.0\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKl2_nU1wTDK",
        "colab_type": "code",
        "outputId": "369ebb34-a30e-4c6a-cf7e-97b3909e827e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "!pip install ipdb"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ipdb\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/01/27427b1f4a97455b345297a48761544bc8e7fb1f3aef6904ec86ddf75f65/ipdb-0.12.3.tar.gz\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from ipdb) (42.0.2)\n",
            "Requirement already satisfied: ipython>=5.1.0 in /usr/local/lib/python3.6/dist-packages (from ipdb) (5.5.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (4.3.3)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (2.1.3)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (0.7.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (4.7.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (4.4.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (1.0.18)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython>=5.1.0->ipdb) (0.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython>=5.1.0->ipdb) (1.12.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=5.1.0->ipdb) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.1.0->ipdb) (0.1.7)\n",
            "Building wheels for collected packages: ipdb\n",
            "  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipdb: filename=ipdb-0.12.3-cp36-none-any.whl size=9223 sha256=fc4503d089beeef99422668ab1dabcf639bcd50b7d5416346da44a28ab4dc0a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/57/43/c5/614153606de8f5e358e266723f53254e70752f4ffc8c85ec63\n",
            "Successfully built ipdb\n",
            "Installing collected packages: ipdb\n",
            "Successfully installed ipdb-0.12.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE4ZnptZDPZB",
        "colab_type": "code",
        "outputId": "7e69c442-fd1e-4df4-dc1d-8b1c2c7f8f66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile preprocess_raw_text.py\n",
        "import argparse\n",
        "import spacy\n",
        "import re\n",
        "from unidecode import unidecode\n",
        "from tqdm import tqdm\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "sentencizer = nlp.create_pipe('sentencizer')\n",
        "nlp.add_pipe(sentencizer, first=True)\n",
        "\n",
        "print(nlp.pipe_names)\n",
        "print(nlp.pipeline)\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--input')\n",
        "    parser.add_argument('--output')\n",
        "    parser.add_argument('--min-len', type=int, default=5, help='minimum sentence length')\n",
        "    parser.add_argument('--max-len', type=int, default=30, help='maximum sentence length')\n",
        "    args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "def sentence_iter(file_):\n",
        "    with open(file_, 'r', errors='ignore') as fin:\n",
        "        cache = []\n",
        "        for line in fin:\n",
        "            line = re.sub('\\s+', ' ', unidecode(line.strip()))\n",
        "            cache.append(line)\n",
        "            if len(cache) == 200:\n",
        "                docs = nlp.pipe(cache)\n",
        "                for doc in docs:\n",
        "                    for s in doc.sents:\n",
        "                        yield s\n",
        "                cache = []\n",
        "        if cache:\n",
        "            docs = nlp.pipe(cache)\n",
        "            for doc in docs:\n",
        "                for s in doc.sents:\n",
        "                    yield s\n",
        "\n",
        "def main(args):\n",
        "    count = 0\n",
        "    print (\"hello\")\n",
        "    #lines = sum(1 for _ in sentence_iter(args.input))\n",
        "    #print (lines)\n",
        "    with open(args.output, 'w') as fout, open(args.input, 'r') as fin:\n",
        "        for s in tqdm(sentence_iter(args.input), total=100000000000):\n",
        "            # s = line.strip().split()\n",
        "            if len(s) >= args.min_len and len(s) <= args.max_len:\n",
        "                l = ['{}|{}'.format(token.text, token.pos_) for token in s]\n",
        "                fout.write(' '.join(l) + '\\n')\n",
        "                count += 1\n",
        "    print (count)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = parse_args()\n",
        "    main(args)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing preprocess_raw_text.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-uKHkZcGFT_",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkmr_ZdzDVA9",
        "colab_type": "code",
        "outputId": "8786eec2-dcb2-469b-87a7-74e551831d68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "!python preprocess_raw_text.py --input books_large_p1.txt --output outp_p1.txt"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['sentencizer', 'tagger']\n",
            "[('sentencizer', <spacy.pipeline.pipes.Sentencizer object at 0x7f5adf434550>), ('tagger', <spacy.pipeline.pipes.Tagger object at 0x7f5adf1e4128>)]\n",
            "hello\n",
            "  0% 40274446/100000000000 [7:27:42<18519:42:52, 1499.30it/s]\n",
            "34299307\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxEQMuK9wMzP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "7d76c75a-ee01-449a-ed59-557b41a40e5e"
      },
      "source": [
        "!head -20 /content/drive/My\\ Drive/Thesis/outp_p1.txt\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the|DET half|ADV -|PUNCT ling|VERB book|NOUN one|NUM in|ADP the|DET fall|NOUN of|ADP igneeria|NOUN series|NOUN kaylee|VERB soderburg|NOUN copyright|NOUN 2013|NUM kaylee|VERB soderburg|NOUN all|DET rights|NOUN reserved|VERB .|PUNCT\n",
            "i|PRON wish|VERB i|PRON had|VERB a|DET better|ADJ answer|NOUN to|ADP that|DET question|NOUN .|PUNCT\n",
            "starlings|NOUN ,|PUNCT new|ADJ york|NOUN is|VERB not|ADV the|DET place|NOUN you|PRON d|AUX expect|VERB much|ADJ to|PART happen|VERB .|PUNCT\n",
            "its|DET a|DET small|ADJ quiet|ADJ town|NOUN ,|PUNCT the|DET kind|NOUN where|ADV everyone|NOUN knows|VERB your|DET name|NOUN .|PUNCT\n",
            "its|DET a|DET place|NOUN where|ADV your|DET parents|NOUN would|AUX nt|ADV even|ADV care|VERB if|ADP you|PRON stayed|VERB out|PART late|ADJ biking|NOUN with|ADP your|DET friends|NOUN .|PUNCT\n",
            "only|ADV because|ADP everyone|NOUN felt|VERB so|ADV safe|ADJ ,|PUNCT so|ADV comfy|ADJ .|PUNCT\n",
            "they|PRON do|VERB nt|ADV know|VERB the|DET half|NOUN of|ADP it|PRON .|PUNCT\n",
            "i|PRON know|VERB it|PRON all|DET and|CCONJ starlings|NOUN is|VERB not|ADV the|DET place|NOUN where|ADV you|PRON want|VERB to|PART be|VERB after|ADP dark|ADJ .|PUNCT\n",
            "the|DET only|ADJ reason|NOUN why|ADV no|DET one|NOUN knows|VERB this|DET is|VERB because|ADP jason|NOUN ,|PUNCT emily|ADV ,|PUNCT seth|ADJ and|CCONJ i|PRON have|VERB kept|VERB it|PRON that|DET way|NOUN .|PUNCT\n",
            "i|PRON walked|VERB along|ADP the|DET empty|ADJ road|NOUN alone|ADV ,|PUNCT occasionally|ADV waving|VERB to|ADP passing|VERB kids|NOUN on|ADP bikes|NOUN .|PUNCT\n",
            "my|DET backpack|NOUN was|VERB slung|VERB over|ADP my|DET shoulder|NOUN ,|PUNCT filled|VERB with|ADP my|DET writing|NOUN books|NOUN and|CCONJ sketchpads|NOUN .|PUNCT\n",
            "i|PRON kept|VERB my|DET eyes|NOUN on|ADP the|DET shadowed|ADJ road|NOUN ,|PUNCT watching|VERB my|DET every|DET step|NOUN .|PUNCT\n",
            "usually|ADV i|PRON was|VERB more|ADV aware|ADJ of|ADP my|DET surroundings|NOUN ,|PUNCT but|CCONJ today|NOUN i|PRON was|VERB tired|ADJ and|CCONJ did|VERB nt|ADV care|VERB if|ADP i|PRON rammed|VERB into|ADP a|DET tree|NOUN .|PUNCT\n",
            "i|PRON kicked|VERB a|DET rock|NOUN into|ADP the|DET grass|NOUN .|PUNCT\n",
            "the|DET sun|NOUN was|VERB starting|VERB to|PART set|VERB ,|PUNCT painting|VERB the|DET sky|NOUN in|ADP brilliant|ADJ oranges|NOUN and|CCONJ reds|NOUN .|PUNCT\n",
            "it|PRON slipped|VERB down|PART the|DET sky|NOUN ,|PUNCT allowing|VERB the|DET first|ADJ stars|NOUN to|PART peek|VERB out|PART from|ADP behind|ADP the|DET bright|ADJ curtain|NOUN .|PUNCT\n",
            "the|DET remaining|VERB light|ADJ cast|NOUN shadows|NOUN over|ADP everything|NOUN ,|PUNCT creating|VERB the|DET illusion|NOUN that|ADP there|ADV was|VERB double|ADJ of|ADP everything|NOUN .|PUNCT\n",
            "the|DET world|NOUN prepared|VERB to|PART go|VERB into|ADP its|DET hours|NOUN of|ADP unreal|ADJ silence|NOUN that|DET made|VERB it|PRON seem|VERB magical|ADJ ,|PUNCT and|CCONJ it|PRON really|ADV was|VERB .|PUNCT\n",
            "my|DET phone|NOUN buzzed|VERB and|CCONJ i|PRON awoke|VERB from|ADP my|DET trance|NOUN .|PUNCT\n",
            "i|PRON thumbed|VERB the|DET keypad|NOUN and|CCONJ opened|VERB the|DET message|NOUN seth|ADJ had|VERB sent|VERB me|PRON .|PUNCT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQKIeyLZsq87",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "3b86c6d5-7494-4ec5-fd51-a5f994befa2f"
      },
      "source": [
        "!head -20 /content/drive/My\\ Drive/Thesis/outp_books_large.txt"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usually|ADV ,|PUNCT he|PRON would|VERB be|VERB tearing|VERB around|ADP the|DET living|NOUN room|NOUN ,|PUNCT playing|VERB with|ADP his|DET toys|NOUN .|PUNCT\n",
            "but|CCONJ just|ADV one|NUM look|NOUN at|ADP a|DET minion|NOUN sent|VERB him|PRON practically|ADV catatonic|ADJ .|PUNCT\n",
            "that|DET had|VERB been|VERB megan|NOUN 's|PART plan|NOUN when|ADV she|PRON got|VERB him|PRON dressed|VERB earlier|ADV .|PUNCT\n",
            "she|PRON liked|VERB to|PART think|VERB being|VERB surrounded|VERB by|ADP adults|NOUN and|CCONJ older|ADJ kids|NOUN was|VERB one|NUM reason|NOUN why|ADV he|PRON was|VERB a|DET such|ADJ a|DET good|ADJ talker|NOUN for|ADP his|DET age|NOUN .|PUNCT\n",
            "`|PUNCT `|PUNCT are|VERB n't|ADV you|PRON being|VERB a|DET good|ADJ boy|NOUN ?|PUNCT ''|PUNCT\n",
            "mason|NOUN barely|ADV acknowledged|VERB her|PRON .|PUNCT\n",
            "instead|ADV ,|PUNCT his|DET baby|NOUN blues|NOUN remained|VERB focused|ADJ on|ADP the|DET television|NOUN .|PUNCT\n",
            "since|ADP the|DET movie|NOUN was|VERB almost|ADV over|ADV ,|PUNCT megan|NOUN knew|VERB she|PRON better|ADV slip|VERB into|ADP the|DET bedroom|NOUN and|CCONJ finish|NOUN getting|VERB ready|ADJ .|PUNCT\n",
            "each|DET time|NOUN she|PRON looked|VERB into|ADP mason|NOUN 's|PART face|NOUN ,|PUNCT she|PRON was|VERB grateful|ADJ that|ADP he|PRON looked|VERB nothing|NOUN like|ADP his|DET father|NOUN .|PUNCT\n",
            "his|DET platinum|NOUN blond|ADJ hair|NOUN and|CCONJ blue|ADJ eyes|NOUN were|VERB completely|ADV hers|NOUN .|PUNCT\n",
            "it|PRON was|VERB only|ADV his|DET build|NOUN that|ADP he|PRON was|VERB taking|VERB after|ADP his|DET father|NOUN .|PUNCT\n",
            "where|ADV megan|NOUN was|VERB a|DET diminutive|ADJ 5'3|NUM ''|PUNCT ,|PUNCT davis|NOUN was|VERB 6'1|NUM ''|PUNCT and|CCONJ two|NUM hundred|NUM pounds|NOUN .|PUNCT\n",
            "mason|NOUN was|VERB already|ADV registering|VERB off|ADP the|DET charts|NOUN in|ADP height|NOUN and|CCONJ weight|NOUN according|VERB to|ADP his|DET pediatrician|NOUN .|PUNCT\n",
            "davis|NOUN had|VERB seen|VERB mason|NOUN only|ADV twice|ADV in|ADP his|DET lifetime|NOUN -|PUNCT the|DET day|NOUN he|PRON had|VERB been|VERB born|VERB and|CCONJ the|DET day|NOUN he|PRON came|VERB home|ADV from|ADP the|DET hospital|NOUN .|PUNCT\n",
            "after|ADP that|DET ,|PUNCT he|PRON had|VERB n't|ADV been|VERB interested|ADJ in|ADP any|DET of|ADP the|DET pictures|NOUN and|CCONJ emails|NOUN megan|NOUN sent|VERB .|PUNCT\n",
            "with|ADP his|DET professional|ADJ football|NOUN career|NOUN on|ADP the|DET rise|NOUN ,|PUNCT davis|NOUN had|VERB n't|ADV wanted|VERB to|PART be|VERB shackled|VERB with|ADP the|DET responsibilities|NOUN of|ADP a|DET baby|NOUN .|PUNCT\n",
            "instead|ADV ,|PUNCT he|PRON wanted|VERB to|PART spend|VERB his|DET time|NOUN off|ADP the|DET field|NOUN partying|VERB until|ADP all|DET hours|NOUN of|ADP the|DET night|NOUN .|PUNCT\n",
            "he|PRON only|ADV paid|VERB child|NOUN support|NOUN when|ADV megan|NOUN threatened|VERB to|PART have|VERB his|DET wages|NOUN garnished|VERB .|PUNCT\n",
            "she|PRON dreaded|VERB the|DET day|NOUN when|ADV mason|NOUN was|VERB old|ADJ enough|ADV to|PART ask|VERB about|ADP his|DET father|NOUN .|PUNCT\n",
            "she|PRON never|ADV wanted|VERB anything|NOUN in|ADP the|DET world|NOUN to|PART hurt|VERB him|PRON ,|PUNCT and|CCONJ she|PRON knew|VERB that|ADP being|VERB rejected|VERB by|ADP his|DET father|NOUN would|VERB .|PUNCT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_THKSAZLsFSd",
        "colab_type": "text"
      },
      "source": [
        "# Preprocess BookCorpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp0GDh4Y3aaY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install fairseq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OxIV62H7ezc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile preprocess_bookcorpus.py # -*- coding: utf-8 -*-\n",
        "\n",
        "import os\n",
        "import codecs\n",
        "import pickle\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "from fairseq.data.dictionary import Dictionary\n",
        "\n",
        "from pungen.utils import sentence_iterator\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--data-dir', type=str, help=\"data directory path to save vocab and processed data\")\n",
        "    parser.add_argument('--vocab', type=str, help=\"corpus path for building vocab\")\n",
        "    parser.add_argument('--corpus', type=str, help=\"corpus path\")\n",
        "    parser.add_argument('--max-dist', type=int, default=5, help=\"maximum distance to the word\")\n",
        "    parser.add_argument('--max-vocab', type=int, default=-1, help=\"maximum number of vocab\")\n",
        "    parser.add_argument('--threshold', type=int, default=-1)\n",
        "    parser.add_argument('--min-dist', type=int, default=0, help=\"minimum distance to the word\")\n",
        "    args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "\n",
        "class Preprocess(object):\n",
        "\n",
        "    def __init__(self, max_dist=5, min_dist=0, data_dir='./data/'):\n",
        "        self.max_dist = max_dist\n",
        "        self.min_dist = min_dist\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "    def get_lemma(self, w):\n",
        "        # w: (token, lemma, tag)\n",
        "        token, lemma, tag = w\n",
        "        return token if lemma == '-PRON-' else lemma\n",
        "\n",
        "    def skipgram(self, sentence, i):\n",
        "        iword = sentence[i]\n",
        "        left = sentence[max(i - self.max_dist, 0) : max(i - self.min_dist, 0)]\n",
        "        right = sentence[i + 1 + self.min_dist : i + 1 + self.max_dist]\n",
        "        n = self.max_dist - self.min_dist\n",
        "        return iword, [self.unk for _ in range(n - len(left))] + left + right + [self.unk for _ in range(n - len(right))]\n",
        "\n",
        "    def build(self, filepath=None, vocab_path=None, threshold=-1, max_vocab=-1):\n",
        "        if vocab_path and os.path.exists(vocab_path):\n",
        "            print(\"loading vocab from {}\".format(vocab_path))\n",
        "            d = Dictionary.load(vocab_path)\n",
        "            print('vocab size {}'.format(len(d)))\n",
        "        else:\n",
        "            print(\"building vocab...\")\n",
        "            d = Dictionary()\n",
        "            for step, line in enumerate(sentence_iterator(filepath)):\n",
        "                if not step % 1000:\n",
        "                    print(\"working on {}kth line\".format(step // 1000), end='\\r')\n",
        "                tokens = [self.get_lemma(w) for w in line]\n",
        "                for tok in tokens:\n",
        "                    d.add_symbol(tok)\n",
        "            d.finalize(threshold=threshold, nwords=max_vocab)\n",
        "            print('build done. vocab size {}'.format(len(d)))\n",
        "            d.save('{}/dict.txt'.format(self.data_dir))\n",
        "\n",
        "        self.vocab = d\n",
        "        self.unk = self.vocab.unk()\n",
        "\n",
        "    def convert(self, filepath):\n",
        "        print(\"converting corpus...\")\n",
        "        step = 0\n",
        "        fout = open('{}/train.bin'.format(args.data_dir), 'wb')\n",
        "        for step, line in enumerate(sentence_iterator(filepath)):\n",
        "            if not step % 1000:\n",
        "                print(\"working on {}kth line\".format(step // 1000), end='\\r')\n",
        "            tokens = [self.get_lemma(w) for w in line]\n",
        "            sent = [self.vocab.index(w) for w in tokens]\n",
        "            if len(sent) <= (self.max_dist - self.min_dist + 1):\n",
        "                continue\n",
        "            for i in range(len(sent)):\n",
        "                iword, owords = self.skipgram(sent, i)\n",
        "                a = np.array([iword] + owords, dtype=np.uint16)\n",
        "                fout.write(a.tobytes())\n",
        "        fout.close()\n",
        "        print(\"conversion done\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = parse_args()\n",
        "    preprocess = Preprocess(max_dist=args.max_dist, min_dist=args.min_dist, data_dir=args.data_dir)\n",
        "    preprocess.build(vocab_path=args.vocab, filepath=args.corpus, threshold=args.threshold, max_vocab=args.max_vocab)\n",
        "    preprocess.convert(args.corpus)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_UMoKu170T7",
        "colab_type": "text"
      },
      "source": [
        "# Train skip-gram model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uMnm8kt76DQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile train_bookcorpus.py\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import argparse\n",
        "import torch as t\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from fairseq.data.dictionary import Dictionary\n",
        "\n",
        "from .model import Word2Vec, SGNS\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--name', type=str, default='sgns', help=\"model name\")\n",
        "    parser.add_argument('--data', type=str, default='./data/', help=\"data directory path\")\n",
        "    parser.add_argument('--vocab', type=str, default='./data/', help=\"data directory path\")\n",
        "    parser.add_argument('--save_dir', type=str, default='./pts/', help=\"model directory path\")\n",
        "    parser.add_argument('--e_dim', type=int, default=300, help=\"embedding dimension\")\n",
        "    parser.add_argument('--n_negs', type=int, default=20, help=\"number of negative samples\")\n",
        "    parser.add_argument('--epoch', type=int, default=100, help=\"number of epochs\")\n",
        "    parser.add_argument('--mb', type=int, default=4096, help=\"mini-batch size\")\n",
        "    parser.add_argument('--ss_t', type=float, default=1e-5, help=\"subsample threshold\")\n",
        "    parser.add_argument('--conti', action='store_true', help=\"continue learning\")\n",
        "    parser.add_argument('--weights', action='store_true', help=\"use weights for negative sampling\")\n",
        "    parser.add_argument('--cuda', action='store_true', help=\"use CUDA\")\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "class PermutedSubsampledCorpus(Dataset):\n",
        "\n",
        "    def __init__(self, datapath, ws=None, window=5):\n",
        "        self.window = window\n",
        "        if ws is not None:\n",
        "            self.data = []\n",
        "            for iword, owords in self.read_data(datapath):\n",
        "                if random.random() > ws[iword]:\n",
        "                    self.data.append((iword, owords))\n",
        "        else:\n",
        "            self.data = [(iword, owords) for iword, owords in self.read_data(datapath)]\n",
        "\n",
        "    def read_data(self, datapath):\n",
        "        n = 2 * self.window + 1\n",
        "        with open(datapath, 'rb') as fin:\n",
        "            print('Reading binary data...')\n",
        "            data = np.fromfile(fin, dtype=np.uint16, count=-1)\n",
        "            for i in range(0, len(data), n):\n",
        "                yield data[i], data[i+1:i+n]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        iword, owords = self.data[idx]\n",
        "        return int(iword), np.array(owords, dtype=np.int)\n",
        "\n",
        "\n",
        "def train(args):\n",
        "    d = Dictionary.load(args.vocab)\n",
        "    wf = np.array(d.count)\n",
        "    wf[wf == 0] = 1\n",
        "    wf = wf / wf.sum()\n",
        "    ws = 1 - np.sqrt(args.ss_t / wf)\n",
        "    ws = np.clip(ws, 0, 1)\n",
        "    vocab_size = len(d)\n",
        "    weights = wf if args.weights else None\n",
        "    if not os.path.isdir(args.save_dir):\n",
        "        os.makedirs(args.save_dir)\n",
        "    model = Word2Vec(vocab_size=vocab_size, embedding_size=args.e_dim)\n",
        "    modelpath = os.path.join(args.save_dir, '{}.pt'.format(args.name))\n",
        "    sgns = SGNS(embedding=model, vocab_size=vocab_size, n_negs=args.n_negs, weights=weights, pad=d.unk())\n",
        "    if os.path.isfile(modelpath) and args.conti:\n",
        "        sgns.load_state_dict(t.load(modelpath))\n",
        "    if args.cuda:\n",
        "        sgns = sgns.cuda()\n",
        "    optim = Adam(sgns.parameters())\n",
        "    optimpath = os.path.join(args.save_dir, '{}.optim.pt'.format(args.name))\n",
        "    if os.path.isfile(optimpath) and args.conti:\n",
        "        optim.load_state_dict(t.load(optimpath))\n",
        "    dataset = PermutedSubsampledCorpus(args.data, ws=ws)\n",
        "    dataloader = DataLoader(dataset, batch_size=args.mb, shuffle=True, num_workers=0)\n",
        "    for epoch in range(1, args.epoch + 1):\n",
        "        total_batches = int(np.ceil(len(dataset) / args.mb))\n",
        "        pbar = tqdm(dataloader)\n",
        "        pbar.set_description(\"[Epoch {}]\".format(epoch))\n",
        "        for iword, owords in pbar:\n",
        "            loss = sgns(iword, owords)\n",
        "            optim.zero_grad()\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "        t.save(sgns.state_dict(), os.path.join(args.save_dir, '{}-e{}.pt'.format(args.name, epoch)))\n",
        "        t.save(optim.state_dict(), os.path.join(args.save_dir, '{}-e{}.optim.pt'.format(args.name, epoch)))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train(parse_args())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wh_NTiulMO2",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# Retriever\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eO8VALnruFLw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile retriever.py\n",
        "import argparse\n",
        "import os, sys\n",
        "import numpy as np\n",
        "import time\n",
        "import pickle\n",
        "from functools import total_ordering\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from .utils import sentence_iterator, Word\n",
        "\n",
        "import logging\n",
        "logger = logging.getLogger('pungen')\n",
        "\n",
        "# define rich comparison ordering methods\n",
        "@total_ordering\n",
        "class Template(object):\n",
        "    def __init__(self, tokens, keyword, id_):\n",
        "        self.id = int(id_)\n",
        "        self.tokens = tokens\n",
        "        self.keyword_positions = [i for i, w in enumerate(tokens) if w == keyword]\n",
        "        self.num_key = len(self.keyword_positions)\n",
        "        # the key_word id is the largest index in keyword_positions, or the last instance of the word\n",
        "        self.keyword_id = None if self.num_key == 0 else max(self.keyword_positions)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokens)\n",
        "\n",
        "    # the swap part of retrieve + swap\n",
        "    def replace_keyword(self, word):\n",
        "        tokens = list(self.tokens)\n",
        "        tokens[self.keyword_id] = word\n",
        "        return tokens\n",
        "\n",
        "    def __str__(self):\n",
        "        return ' '.join(['[{}]'.format(w) if i == self.keyword_id else w for i, w in enumerate(self.tokens)])\n",
        "\n",
        "    # for ranking the words by the placement in the sentence\n",
        "    def __lt__(self, other):\n",
        "        # Containing keyword is better\n",
        "        if self.num_key == 0:\n",
        "            return True\n",
        "        # Fewer keywords is better\n",
        "        if self.num_key > other.num_key:\n",
        "            return True\n",
        "        # Later keywords is better\n",
        "        if self.keyword_id < other.keyword_id:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        if self.num_key == 0 and other.num_key == 0:\n",
        "            return True\n",
        "        if self.num_key == other.num_key and self.keyword_id == other.keyword_id:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "\n",
        "class Retriever(object):\n",
        "    def __init__(self, doc_files, path=None, overwrite=False):\n",
        "        logger.info('reading retriever docs from {}'.format(' '.join(doc_files)))\n",
        "        self.docs = [line.strip() for line in open(doc_files[0], 'r')]\n",
        "\n",
        "        if overwrite or (path is None or not os.path.exists(path)):\n",
        "            logger.info('building retriever index')\n",
        "            self.vectorizer = TfidfVectorizer(analyzer=str.split)\n",
        "            self.tfidf_matrix = self.vectorizer.fit_transform(self.docs)\n",
        "            if path is not None:\n",
        "                self.save(path)\n",
        "        else:\n",
        "            logger.info('loading retriever index from {}'.format(path))\n",
        "            with open(path, 'rb') as fin:\n",
        "                obj = pickle.load(fin)\n",
        "                self.vectorizer = obj['vectorizer']\n",
        "                self.tfidf_matrix = obj['tfidf_mat']\n",
        "\n",
        "    def save(self, path):\n",
        "        with open(path, 'wb') as fout:\n",
        "            obj = {\n",
        "                    'vectorizer': self.vectorizer,\n",
        "                    'tfidf_mat': self.tfidf_matrix,\n",
        "                    }\n",
        "            pickle.dump(obj, fout)\n",
        "\n",
        "    def query(self, keywords, k=1):\n",
        "        features = self.vectorizer.transform([keywords])\n",
        "        scores = self.tfidf_matrix * features.T\n",
        "        scores = scores.todense()\n",
        "        scores = np.squeeze(np.array(scores), axis=1)\n",
        "        ids = np.argsort(scores)[-k:][::-1]\n",
        "        return ids\n",
        "\n",
        "    def valid_template(self, template):\n",
        "        return template.num_key == 1\n",
        "\n",
        "    def retrieve_pun_template(self, alter_word, len_threshold=10, pos_threshold=0.5, num_cands=500, num_templates=None):\n",
        "        ids = self.query(alter_word, num_cands)\n",
        "        templates = [Template(self.docs[id_].split(), alter_word, id_) for id_ in ids]\n",
        "        templates = [t for t in templates if t.num_key > 0 and len(t.tokens) > len_threshold]\n",
        "        if len(templates) == 0:\n",
        "            logger.info('FAIL: no retrieved sentence contains the keyword {}.'.format(alter_word))\n",
        "            return []\n",
        "\n",
        "        valid_templates = [t for t in templates if self.valid_template(t)]\n",
        "        if len(valid_templates) == 0:\n",
        "            valid_templates = templates\n",
        "        templates = sorted(valid_templates, reverse=True)[:num_templates]\n",
        "        return templates\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--doc-file', nargs='+', help='training corpus')\n",
        "    parser.add_argument('--interactive', action='store_true')\n",
        "    parser.add_argument('--path', default='models/retriever.pkl', help='retriever model path')\n",
        "    parser.add_argument('--overwrite', action='store_true')\n",
        "    parser.add_argument('--keywords', help='file containing keywords')\n",
        "    parser.add_argument('--alterwords', help='file containing alternative words')\n",
        "    parser.add_argument('--outfile', help='output file for the retrieved sentences')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    retriever = Retriever(args.doc_file, args.path, args.overwrite)\n",
        "\n",
        "    if args.interactive:\n",
        "        while True:\n",
        "            alter_word, pun_word = input('Keywords:\\n').split()\n",
        "            alter_sents, pun_sents, pun_word_ids, alter_ori_sents = retriever.retrieve_pun_template(alter_word, num_cands=100)\n",
        "            for ori_sent, sent in zip(alter_ori_sents, alter_sents):\n",
        "                print(' '.join(sent))\n",
        "                print(' '.join(ori_sent))\n",
        "            if not alter_sents:\n",
        "                print('No candidates found')\n",
        "    elif args.outfile:\n",
        "        with open(args.keywords, 'r') as fin, open(args.alterwords, 'r') as afin, open(args.outfile, 'w') as outf:\n",
        "            for key, alter in zip(fin, afin):\n",
        "                key = key.strip()\n",
        "                alter = alter.strip()\n",
        "                ids = retriever.query(key, 10)\n",
        "                contents = [retriever.ori_docs[id_] for id_ in ids]\n",
        "                contents.sort(key = lambda s: len(s), reverse=True)\n",
        "#                print(line.strip())\n",
        "                for ct in contents:\n",
        "                    ct_list = ct.split()\n",
        "                    try:\n",
        "                        idx = ct_list.index(key)\n",
        "                    except:\n",
        "                        sys.stderr.write('cannot find the word %s!\\n' % key)\n",
        "                        continue\n",
        "                    ct_list[idx] = alter\n",
        "                    outf.write(ct.lower() + '\\n')\n",
        "                    print(' '.join(ct_list).lower())\n",
        "                    break\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YYvvh-f8W5R",
        "colab_type": "text"
      },
      "source": [
        "# Preprocess for edit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B5XS2UE8eli",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile preprocess_pungen.py\n",
        "#!/usr/bin/env python3\n",
        "# Copyright (c) 2017-present, Facebook, Inc.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the license found in the LICENSE file in\n",
        "# the root directory of this source tree. An additional grant of patent rights\n",
        "# can be found in the PATENTS file in the same directory.\n",
        "\"\"\"\n",
        "Data pre-processing: build vocabularies and binarize training data.\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "from collections import Counter\n",
        "from itertools import zip_longest\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "from fairseq.data import indexed_dataset, EditDictionary as Dictionary\n",
        "from fairseq.tokenizer import Tokenizer, tokenize_line\n",
        "from multiprocessing import Pool, Manager, Process\n",
        "\n",
        "\n",
        "def get_parser():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('-s', '--source-lang', default=None, metavar='SRC', help='source language')\n",
        "    parser.add_argument('-t', '--target-lang', default=None, metavar='TARGET', help='target language')\n",
        "    parser.add_argument('--trainpref', metavar='FP', default=None, help='train file prefix')\n",
        "    parser.add_argument('--validpref', metavar='FP', default=None, help='comma separated, valid file prefixes')\n",
        "    parser.add_argument('--testpref', metavar='FP', default=None, help='comma separated, test file prefixes')\n",
        "    parser.add_argument('--destdir', metavar='DIR', default='data-bin', help='destination dir')\n",
        "    parser.add_argument('--thresholdtgt', metavar='N', default=0, type=int,\n",
        "                        help='map words appearing less than threshold times to unknown')\n",
        "    parser.add_argument('--thresholdsrc', metavar='N', default=0, type=int,\n",
        "                        help='map words appearing less than threshold times to unknown')\n",
        "    parser.add_argument('--tgtdict', metavar='FP', help='reuse given target dictionary')\n",
        "    parser.add_argument('--srcdict', metavar='FP', help='reuse given source dictionary')\n",
        "    parser.add_argument('--nwordstgt', metavar='N', default=-1, type=int, help='number of target words to retain')\n",
        "    parser.add_argument('--nwordssrc', metavar='N', default=-1, type=int, help='number of source words to retain')\n",
        "    parser.add_argument('--alignfile', metavar='ALIGN', default=None, help='an alignment file (optional)')\n",
        "    parser.add_argument('--output-format', metavar='FORMAT', default='binary', choices=['binary', 'raw'],\n",
        "                        help='output format (optional)')\n",
        "    parser.add_argument('--joined-dictionary', action='store_true', help='Generate joined dictionary')\n",
        "    parser.add_argument('--only-source', action='store_true', help='Only process the source language')\n",
        "    parser.add_argument('--padding-factor', metavar='N', default=8, type=int,\n",
        "                        help='Pad dictionary size to be multiple of N')\n",
        "    parser.add_argument('--workers', metavar='N', default=1, type=int, help='number of parallel workers')\n",
        "    return parser\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    print(args)\n",
        "    os.makedirs(args.destdir, exist_ok=True)\n",
        "    target = not args.only_source\n",
        "\n",
        "    def build_dictionary(filenames):\n",
        "        d = Dictionary()\n",
        "        for filename in filenames:\n",
        "            Tokenizer.add_file_to_dictionary(filename, d, tokenize_line, args.workers)\n",
        "        return d\n",
        "\n",
        "    def train_path(lang):\n",
        "        return '{}{}'.format(args.trainpref, ('.' + lang) if lang else '')\n",
        "\n",
        "    def file_name(prefix, lang):\n",
        "        fname = prefix\n",
        "        if lang is not None:\n",
        "            fname += f'.{lang}'\n",
        "        return fname\n",
        "\n",
        "    def dest_path(prefix, lang):\n",
        "        return os.path.join(args.destdir, file_name(prefix, lang))\n",
        "\n",
        "    def dict_path(lang):\n",
        "        return dest_path('dict', lang) + '.txt'\n",
        "\n",
        "    if args.joined_dictionary:\n",
        "        assert not args.srcdict, 'cannot combine --srcdict and --joined-dictionary'\n",
        "        assert not args.tgtdict, 'cannot combine --tgtdict and --joined-dictionary'\n",
        "        src_dict = build_dictionary(set([\n",
        "            train_path(lang)\n",
        "            for lang in [args.source_lang, args.target_lang]\n",
        "        ]))\n",
        "        tgt_dict = src_dict\n",
        "    else:\n",
        "        if args.srcdict:\n",
        "            src_dict = Dictionary.load(args.srcdict)\n",
        "        else:\n",
        "            assert args.trainpref, \"--trainpref must be set if --srcdict is not specified\"\n",
        "            src_dict = build_dictionary([train_path(args.source_lang)])\n",
        "        if target:\n",
        "            if args.tgtdict:\n",
        "                tgt_dict = Dictionary.load(args.tgtdict)\n",
        "            else:\n",
        "                assert args.trainpref, \"--trainpref must be set if --tgtdict is not specified\"\n",
        "                tgt_dict = build_dictionary([train_path(args.target_lang)])\n",
        "\n",
        "    src_dict.finalize(\n",
        "        threshold=args.thresholdsrc,\n",
        "        nwords=args.nwordssrc,\n",
        "        padding_factor=args.padding_factor,\n",
        "    )\n",
        "    src_dict.save(dict_path(args.source_lang))\n",
        "    if target:\n",
        "        if not args.joined_dictionary:\n",
        "            tgt_dict.finalize(\n",
        "                threshold=args.thresholdtgt,\n",
        "                nwords=args.nwordstgt,\n",
        "                padding_factor=args.padding_factor,\n",
        "            )\n",
        "        tgt_dict.save(dict_path(args.target_lang))\n",
        "\n",
        "    def make_binary_dataset(input_prefix, output_prefix, lang, num_workers):\n",
        "        dict = Dictionary.load(dict_path(lang))\n",
        "        print('| [{}] Dictionary: {} types'.format(lang, len(dict) - 1))\n",
        "        n_seq_tok = [0, 0]\n",
        "        replaced = Counter()\n",
        "\n",
        "        def merge_result(worker_result):\n",
        "            replaced.update(worker_result['replaced'])\n",
        "            n_seq_tok[0] += worker_result['nseq']\n",
        "            n_seq_tok[1] += worker_result['ntok']\n",
        "\n",
        "        input_file = '{}{}'.format(input_prefix, ('.' + lang) if lang is not None else '')\n",
        "        offsets = Tokenizer.find_offsets(input_file, num_workers)\n",
        "        pool = None\n",
        "        if num_workers > 1:\n",
        "            pool = Pool(processes=num_workers-1)\n",
        "            for worker_id in range(1, num_workers):\n",
        "                prefix = \"{}{}\".format(output_prefix, worker_id)\n",
        "                pool.apply_async(binarize, (args, input_file, dict, prefix, lang,\n",
        "                                            offsets[worker_id],\n",
        "                                            offsets[worker_id + 1]), callback=merge_result)\n",
        "            pool.close()\n",
        "\n",
        "        ds = indexed_dataset.IndexedDatasetBuilder(dataset_dest_file(args, output_prefix, lang, 'bin'))\n",
        "        merge_result(Tokenizer.binarize(input_file, dict, lambda t: ds.add_item(t),\n",
        "                                        offset=0, end=offsets[1]))\n",
        "        if num_workers > 1:\n",
        "            pool.join()\n",
        "            for worker_id in range(1, num_workers):\n",
        "                prefix = \"{}{}\".format(output_prefix, worker_id)\n",
        "                temp_file_path = dataset_dest_prefix(args, prefix, lang)\n",
        "                ds.merge_file_(temp_file_path)\n",
        "                os.remove(indexed_dataset.data_file_path(temp_file_path))\n",
        "                os.remove(indexed_dataset.index_file_path(temp_file_path))\n",
        "\n",
        "\n",
        "        ds.finalize(dataset_dest_file(args, output_prefix, lang, 'idx'))\n",
        "\n",
        "\n",
        "        print('| [{}] {}: {} sents, {} tokens, {:.3}% replaced by {}'.format(\n",
        "            lang, input_file, n_seq_tok[0], n_seq_tok[1],\n",
        "            100 * sum(replaced.values()) / n_seq_tok[1], dict.unk_word))\n",
        "\n",
        "\n",
        "\n",
        "    def make_dataset(input_prefix, output_prefix, lang, num_workers=1):\n",
        "        if args.output_format == 'binary':\n",
        "            make_binary_dataset(input_prefix, output_prefix, lang, num_workers)\n",
        "        elif args.output_format == 'raw':\n",
        "            # Copy original text file to destination folder\n",
        "            output_text_file = dest_path(\n",
        "                output_prefix + '.{}-{}'.format(args.source_lang, args.target_lang),\n",
        "                lang,\n",
        "            )\n",
        "            shutil.copyfile(file_name(input_prefix, lang), output_text_file)\n",
        "\n",
        "    def make_all(lang):\n",
        "        if args.trainpref:\n",
        "            make_dataset(args.trainpref, 'train', lang, num_workers=args.workers)\n",
        "        if args.validpref:\n",
        "            for k, validpref in enumerate(args.validpref.split(',')):\n",
        "                outprefix = 'valid{}'.format(k) if k > 0 else 'valid'\n",
        "                make_dataset(validpref, outprefix, lang)\n",
        "        if args.testpref:\n",
        "            for k, testpref in enumerate(args.testpref.split(',')):\n",
        "                outprefix = 'test{}'.format(k) if k > 0 else 'test'\n",
        "                make_dataset(testpref, outprefix, lang)\n",
        "\n",
        "    make_all(args.source_lang)\n",
        "    if target:\n",
        "        make_all(args.target_lang)\n",
        "\n",
        "    print('| Wrote preprocessed data to {}'.format(args.destdir))\n",
        "\n",
        "    if args.alignfile:\n",
        "        assert args.trainpref, \"--trainpref must be set if --alignfile is specified\"\n",
        "        src_file_name = train_path(args.source_lang)\n",
        "        tgt_file_name = train_path(args.target_lang)\n",
        "        src_dict = Dictionary.load(dict_path(args.source_lang))\n",
        "        tgt_dict = Dictionary.load(dict_path(args.target_lang))\n",
        "        freq_map = {}\n",
        "        with open(args.alignfile, 'r') as align_file:\n",
        "            with open(src_file_name, 'r') as src_file:\n",
        "                with open(tgt_file_name, 'r') as tgt_file:\n",
        "                    for a, s, t in zip_longest(align_file, src_file, tgt_file):\n",
        "                        si = Tokenizer.tokenize(s, src_dict, add_if_not_exist=False)\n",
        "                        ti = Tokenizer.tokenize(t, tgt_dict, add_if_not_exist=False)\n",
        "                        ai = list(map(lambda x: tuple(x.split('-')), a.split()))\n",
        "                        for sai, tai in ai:\n",
        "                            srcidx = si[int(sai)]\n",
        "                            tgtidx = ti[int(tai)]\n",
        "                            if srcidx != src_dict.unk() and tgtidx != tgt_dict.unk():\n",
        "                                assert srcidx != src_dict.pad()\n",
        "                                assert srcidx != src_dict.eos()\n",
        "                                assert tgtidx != tgt_dict.pad()\n",
        "                                assert tgtidx != tgt_dict.eos()\n",
        "\n",
        "                                if srcidx not in freq_map:\n",
        "                                    freq_map[srcidx] = {}\n",
        "                                if tgtidx not in freq_map[srcidx]:\n",
        "                                    freq_map[srcidx][tgtidx] = 1\n",
        "                                else:\n",
        "                                    freq_map[srcidx][tgtidx] += 1\n",
        "\n",
        "        align_dict = {}\n",
        "        for srcidx in freq_map.keys():\n",
        "            align_dict[srcidx] = max(freq_map[srcidx], key=freq_map[srcidx].get)\n",
        "\n",
        "        with open(os.path.join(args.destdir, 'alignment.{}-{}.txt'.format(\n",
        "                args.source_lang, args.target_lang)), 'w') as f:\n",
        "            for k, v in align_dict.items():\n",
        "                print('{} {}'.format(src_dict[k], tgt_dict[v]), file=f)\n",
        "\n",
        "\n",
        "\n",
        "def binarize(args, filename, dict, output_prefix, lang, offset, end):\n",
        "\n",
        "    ds = indexed_dataset.IndexedDatasetBuilder(dataset_dest_file(args, output_prefix, lang, 'bin'))\n",
        "    def consumer(tensor):\n",
        "        ds.add_item(tensor)\n",
        "\n",
        "    res = Tokenizer.binarize(filename, dict, consumer, offset=offset, end=end)\n",
        "    ds.finalize(dataset_dest_file(args, output_prefix, lang, 'idx'))\n",
        "    return res\n",
        "\n",
        "def dataset_dest_prefix(args, output_prefix, lang):\n",
        "    base = f'{args.destdir}/{output_prefix}'\n",
        "    lang_part = f'.{args.source_lang}-{args.target_lang}.{lang}' if lang is not None else ''\n",
        "    return f'{base}{lang_part}'\n",
        "\n",
        "\n",
        "def dataset_dest_file(args, output_prefix, lang, extension):\n",
        "    base = dataset_dest_prefix(args, output_prefix, lang)\n",
        "    return f'{base}.{extension}'\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = get_parser()\n",
        "    args = parser.parse_args()\n",
        "    main(args)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPTs1wN_8rkm",
        "colab_type": "text"
      },
      "source": [
        "# Training for edit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QY5Icm0E8vD-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python3 -u\n",
        "# Copyright (c) 2017-present, Facebook, Inc.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the license found in the LICENSE file in\n",
        "# the root directory of this source tree. An additional grant of patent rights\n",
        "# can be found in the PATENTS file in the same directory.\n",
        "\"\"\"\n",
        "Train a new model on one or across multiple GPUs.\n",
        "\"\"\"\n",
        "\n",
        "import collections\n",
        "import itertools\n",
        "import os\n",
        "import math\n",
        "import torch\n",
        "\n",
        "from fairseq import distributed_utils, options, progress_bar, tasks, utils\n",
        "from fairseq.data import iterators\n",
        "from fairseq.trainer import Trainer\n",
        "from fairseq.meters import AverageMeter, StopwatchMeter\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    if args.max_tokens is None:\n",
        "        args.max_tokens = 6000\n",
        "    print(args)\n",
        "\n",
        "    if not torch.cuda.is_available():\n",
        "        raise NotImplementedError('Training on CPU is not supported')\n",
        "    torch.cuda.set_device(args.device_id)\n",
        "    torch.manual_seed(args.seed)\n",
        "\n",
        "    # Setup task, e.g., translation, language modeling, etc.\n",
        "    task = tasks.setup_task(args)\n",
        "\n",
        "    # Load dataset splits\n",
        "    load_dataset_splits(task, ['train', 'valid'])\n",
        "\n",
        "    # Build model and criterion\n",
        "    model = task.build_model(args)\n",
        "    criterion = task.build_criterion(args)\n",
        "    print('| model {}, criterion {}'.format(args.arch, criterion.__class__.__name__))\n",
        "    print('| num. model params: {}'.format(sum(p.numel() for p in model.parameters())))\n",
        "\n",
        "    # Make a dummy batch to (i) warm the caching allocator and (ii) as a\n",
        "    # placeholder DistributedDataParallel when there's an uneven number of\n",
        "    # batches per worker.\n",
        "    max_positions = utils.resolve_max_positions(\n",
        "        task.max_positions(),\n",
        "        model.max_positions(),\n",
        "    )\n",
        "    dummy_batch = task.dataset('train').get_dummy_batch(args.max_tokens, max_positions)\n",
        "\n",
        "    # Build trainer\n",
        "    trainer = Trainer(args, task, model, criterion, dummy_batch)\n",
        "    print('| training on {} GPUs'.format(args.distributed_world_size))\n",
        "    print('| max tokens per GPU = {} and max sentences per GPU = {}'.format(\n",
        "        args.max_tokens,\n",
        "        args.max_sentences,\n",
        "    ))\n",
        "\n",
        "    # Initialize dataloader\n",
        "    epoch_itr = task.get_batch_iterator(\n",
        "        dataset=task.dataset(args.train_subset),\n",
        "        max_tokens=args.max_tokens,\n",
        "        max_sentences=args.max_sentences,\n",
        "        max_positions=max_positions,\n",
        "        ignore_invalid_inputs=True,\n",
        "        required_batch_size_multiple=8,\n",
        "        seed=args.seed,\n",
        "        num_shards=args.distributed_world_size,\n",
        "        shard_id=args.distributed_rank,\n",
        "    )\n",
        "\n",
        "    # Load the latest checkpoint if one is available\n",
        "    if not load_checkpoint(args, trainer, epoch_itr):\n",
        "        trainer.dummy_train_step([dummy_batch])\n",
        "\n",
        "    # Train until the learning rate gets too small\n",
        "    max_epoch = args.max_epoch or math.inf\n",
        "    max_update = args.max_update or math.inf\n",
        "    lr = trainer.get_lr()\n",
        "    train_meter = StopwatchMeter()\n",
        "    train_meter.start()\n",
        "    valid_losses = [None]\n",
        "    valid_subsets = args.valid_subset.split(',')\n",
        "    while lr > args.min_lr and epoch_itr.epoch < max_epoch and trainer.get_num_updates() < max_update:\n",
        "        # train for one epoch\n",
        "        train(args, trainer, task, epoch_itr)\n",
        "\n",
        "        if epoch_itr.epoch % args.validate_interval == 0:\n",
        "            valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)\n",
        "\n",
        "        # only use first validation loss to update the learning rate\n",
        "        lr = trainer.lr_step(epoch_itr.epoch, valid_losses[0])\n",
        "\n",
        "        # save checkpoint\n",
        "        if epoch_itr.epoch % args.save_interval == 0:\n",
        "            save_checkpoint(args, trainer, epoch_itr, valid_losses[0])\n",
        "    train_meter.stop()\n",
        "    print('| done training in {:.1f} seconds'.format(train_meter.sum))\n",
        "\n",
        "\n",
        "def train(args, trainer, task, epoch_itr):\n",
        "    \"\"\"Train the model for one epoch.\"\"\"\n",
        "\n",
        "    # Update parameters every N batches\n",
        "    if epoch_itr.epoch <= len(args.update_freq):\n",
        "        update_freq = args.update_freq[epoch_itr.epoch - 1]\n",
        "    else:\n",
        "        update_freq = args.update_freq[-1]\n",
        "\n",
        "    # Initialize data iterator\n",
        "    itr = epoch_itr.next_epoch_itr(fix_batches_to_gpus=args.fix_batches_to_gpus)\n",
        "    itr = iterators.GroupedIterator(itr, update_freq)\n",
        "    progress = progress_bar.build_progress_bar(\n",
        "        args, itr, epoch_itr.epoch, no_progress_bar='simple',\n",
        "    )\n",
        "\n",
        "    extra_meters = collections.defaultdict(lambda: AverageMeter())\n",
        "    first_valid = args.valid_subset.split(',')[0]\n",
        "    max_update = args.max_update or math.inf\n",
        "    for i, samples in enumerate(progress, start=epoch_itr.iterations_in_epoch):\n",
        "        log_output = trainer.train_step(samples)\n",
        "        if log_output is None:\n",
        "            continue\n",
        "\n",
        "        # log mid-epoch stats\n",
        "        stats = get_training_stats(trainer)\n",
        "        for k, v in log_output.items():\n",
        "            if k in ['loss', 'nll_loss', 'ntokens', 'nsentences', 'sample_size']:\n",
        "                continue  # these are already logged above\n",
        "            if 'loss' in k:\n",
        "                extra_meters[k].update(v, log_output['sample_size'])\n",
        "            else:\n",
        "                extra_meters[k].update(v)\n",
        "            stats[k] = extra_meters[k].avg\n",
        "        progress.log(stats)\n",
        "\n",
        "        # ignore the first mini-batch in words-per-second calculation\n",
        "        if i == 0:\n",
        "            trainer.get_meter('wps').reset()\n",
        "\n",
        "        num_updates = trainer.get_num_updates()\n",
        "        if args.save_interval_updates > 0 and num_updates % args.save_interval_updates == 0 and num_updates > 0:\n",
        "            valid_losses = validate(args, trainer, task, epoch_itr, [first_valid])\n",
        "            save_checkpoint(args, trainer, epoch_itr, valid_losses[0])\n",
        "\n",
        "        if num_updates >= max_update:\n",
        "            break\n",
        "\n",
        "    # log end-of-epoch stats\n",
        "    stats = get_training_stats(trainer)\n",
        "    for k, meter in extra_meters.items():\n",
        "        stats[k] = meter.avg\n",
        "    progress.print(stats)\n",
        "\n",
        "    # reset training meters\n",
        "    for k in [\n",
        "        'train_loss', 'train_nll_loss', 'wps', 'ups', 'wpb', 'bsz', 'gnorm', 'clip',\n",
        "    ]:\n",
        "        meter = trainer.get_meter(k)\n",
        "        if meter is not None:\n",
        "            meter.reset()\n",
        "\n",
        "\n",
        "def get_training_stats(trainer):\n",
        "    stats = collections.OrderedDict()\n",
        "    stats['loss'] = '{:.3f}'.format(trainer.get_meter('train_loss').avg)\n",
        "    if trainer.get_meter('train_nll_loss').count > 0:\n",
        "        nll_loss = trainer.get_meter('train_nll_loss').avg\n",
        "        stats['nll_loss'] = '{:.3f}'.format(nll_loss)\n",
        "    else:\n",
        "        nll_loss = trainer.get_meter('train_loss').avg\n",
        "    stats['ppl'] = get_perplexity(nll_loss)\n",
        "    stats['wps'] = round(trainer.get_meter('wps').avg)\n",
        "    stats['ups'] = '{:.1f}'.format(trainer.get_meter('ups').avg)\n",
        "    stats['wpb'] = round(trainer.get_meter('wpb').avg)\n",
        "    stats['bsz'] = round(trainer.get_meter('bsz').avg)\n",
        "    stats['num_updates'] = trainer.get_num_updates()\n",
        "    stats['lr'] = trainer.get_lr()\n",
        "    stats['gnorm'] = '{:.3f}'.format(trainer.get_meter('gnorm').avg)\n",
        "    stats['clip'] = '{:.0%}'.format(trainer.get_meter('clip').avg)\n",
        "    stats['oom'] = trainer.get_meter('oom').avg\n",
        "    if trainer.get_meter('loss_scale') is not None:\n",
        "        stats['loss_scale'] = '{:.3f}'.format(trainer.get_meter('loss_scale').avg)\n",
        "    stats['wall'] = round(trainer.get_meter('wall').elapsed_time)\n",
        "    stats['train_wall'] = round(trainer.get_meter('train_wall').sum)\n",
        "    return stats\n",
        "\n",
        "\n",
        "def validate(args, trainer, task, epoch_itr, subsets):\n",
        "    \"\"\"Evaluate the model on the validation set(s) and return the losses.\"\"\"\n",
        "    valid_losses = []\n",
        "    for subset in subsets:\n",
        "        # Initialize data iterator\n",
        "        itr = task.get_batch_iterator(\n",
        "            dataset=task.dataset(subset),\n",
        "            max_tokens=args.max_tokens,\n",
        "            max_sentences=args.max_sentences_valid,\n",
        "            max_positions=utils.resolve_max_positions(\n",
        "                task.max_positions(),\n",
        "                trainer.get_model().max_positions(),\n",
        "            ),\n",
        "            ignore_invalid_inputs=args.skip_invalid_size_inputs_valid_test,\n",
        "            required_batch_size_multiple=8,\n",
        "            seed=args.seed,\n",
        "            num_shards=args.distributed_world_size,\n",
        "            shard_id=args.distributed_rank,\n",
        "        ).next_epoch_itr(shuffle=False)\n",
        "        progress = progress_bar.build_progress_bar(\n",
        "            args, itr, epoch_itr.epoch,\n",
        "            prefix='valid on \\'{}\\' subset'.format(subset),\n",
        "            no_progress_bar='simple'\n",
        "        )\n",
        "\n",
        "        # reset validation loss meters\n",
        "        for k in ['valid_loss', 'valid_nll_loss']:\n",
        "            meter = trainer.get_meter(k)\n",
        "            if meter is not None:\n",
        "                meter.reset()\n",
        "        extra_meters = collections.defaultdict(lambda: AverageMeter())\n",
        "\n",
        "        for sample in progress:\n",
        "            log_output = trainer.valid_step(sample)\n",
        "\n",
        "            for k, v in log_output.items():\n",
        "                if k in ['loss', 'nll_loss', 'ntokens', 'nsentences', 'sample_size']:\n",
        "                    continue\n",
        "                extra_meters[k].update(v)\n",
        "\n",
        "        # log validation stats\n",
        "        stats = get_valid_stats(trainer)\n",
        "        for k, meter in extra_meters.items():\n",
        "            stats[k] = meter.avg\n",
        "        progress.print(stats)\n",
        "\n",
        "        valid_losses.append(stats['valid_loss'])\n",
        "    return valid_losses\n",
        "\n",
        "\n",
        "def get_valid_stats(trainer):\n",
        "    stats = collections.OrderedDict()\n",
        "    stats['valid_loss'] = trainer.get_meter('valid_loss').avg\n",
        "    if trainer.get_meter('valid_nll_loss').count > 0:\n",
        "        nll_loss = trainer.get_meter('valid_nll_loss').avg\n",
        "        stats['valid_nll_loss'] = nll_loss\n",
        "    else:\n",
        "        nll_loss = trainer.get_meter('valid_loss').avg\n",
        "    stats['valid_ppl'] = get_perplexity(nll_loss)\n",
        "    stats['num_updates'] = trainer.get_num_updates()\n",
        "    if hasattr(save_checkpoint, 'best'):\n",
        "        stats['best'] = min(save_checkpoint.best, stats['valid_loss'])\n",
        "    return stats\n",
        "\n",
        "\n",
        "def get_perplexity(loss):\n",
        "    try:\n",
        "        return '{:.2f}'.format(math.pow(2, loss))\n",
        "    except OverflowError:\n",
        "        return float('inf')\n",
        "\n",
        "\n",
        "def save_checkpoint(args, trainer, epoch_itr, val_loss):\n",
        "    if args.no_save or not distributed_utils.is_master(args):\n",
        "        return\n",
        "    epoch = epoch_itr.epoch\n",
        "    end_of_epoch = epoch_itr.end_of_epoch()\n",
        "    updates = trainer.get_num_updates()\n",
        "\n",
        "    checkpoint_conds = collections.OrderedDict()\n",
        "    checkpoint_conds['checkpoint{}.pt'.format(epoch)] = (\n",
        "            end_of_epoch and not args.no_epoch_checkpoints and\n",
        "            epoch % args.save_interval == 0\n",
        "    )\n",
        "    checkpoint_conds['checkpoint_{}_{}.pt'.format(epoch, updates)] = (\n",
        "            not end_of_epoch and args.save_interval_updates > 0 and\n",
        "            updates % args.save_interval_updates == 0\n",
        "    )\n",
        "    checkpoint_conds['checkpoint_best.pt'] = (\n",
        "            val_loss is not None and\n",
        "            (not hasattr(save_checkpoint, 'best') or val_loss < save_checkpoint.best)\n",
        "    )\n",
        "    checkpoint_conds['checkpoint_last.pt'] = True  # keep this last so that it's a symlink\n",
        "\n",
        "    prev_best = getattr(save_checkpoint, 'best', val_loss)\n",
        "    if val_loss is not None:\n",
        "        save_checkpoint.best = min(val_loss, prev_best)\n",
        "    extra_state = {\n",
        "        'best': save_checkpoint.best,\n",
        "        'train_iterator': epoch_itr.state_dict(),\n",
        "        'val_loss': val_loss,\n",
        "    }\n",
        "\n",
        "    checkpoints = [os.path.join(args.save_dir, fn) for fn, cond in checkpoint_conds.items() if cond]\n",
        "    if len(checkpoints) > 0:\n",
        "        for cp in checkpoints:\n",
        "            trainer.save_checkpoint(cp, extra_state)\n",
        "\n",
        "    if not end_of_epoch and args.keep_interval_updates > 0:\n",
        "        # remove old checkpoints; checkpoints are sorted in descending order\n",
        "        checkpoints = utils.checkpoint_paths(args.save_dir, pattern=r'checkpoint_\\d+_(\\d+)\\.pt')\n",
        "        for old_chk in checkpoints[args.keep_interval_updates:]:\n",
        "            os.remove(old_chk)\n",
        "\n",
        "\n",
        "def load_checkpoint(args, trainer, epoch_itr):\n",
        "    \"\"\"Load a checkpoint and replay dataloader to match.\"\"\"\n",
        "    os.makedirs(args.save_dir, exist_ok=True)\n",
        "    checkpoint_path = os.path.join(args.save_dir, args.restore_file)\n",
        "    if os.path.isfile(checkpoint_path):\n",
        "        extra_state = trainer.load_checkpoint(checkpoint_path, args.reset_optimizer, args.reset_lr_scheduler,\n",
        "                                              eval(args.optimizer_overrides))\n",
        "        if extra_state is not None:\n",
        "            # replay train iterator to match checkpoint\n",
        "            epoch_itr.load_state_dict(extra_state['train_iterator'])\n",
        "\n",
        "            print('| loaded checkpoint {} (epoch {} @ {} updates)'.format(\n",
        "                checkpoint_path, epoch_itr.epoch, trainer.get_num_updates()))\n",
        "\n",
        "            trainer.lr_step(epoch_itr.epoch)\n",
        "            trainer.lr_step_update(trainer.get_num_updates())\n",
        "            if 'best' in extra_state:\n",
        "                save_checkpoint.best = extra_state['best']\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def load_dataset_splits(task, splits):\n",
        "    for split in splits:\n",
        "        if split == 'train':\n",
        "            task.load_dataset(split, combine=True)\n",
        "        else:\n",
        "            for k in itertools.count():\n",
        "                split_k = split + (str(k) if k > 0 else '')\n",
        "                try:\n",
        "                    task.load_dataset(split_k, combine=False)\n",
        "                except FileNotFoundError as e:\n",
        "                    if k > 0:\n",
        "                        break\n",
        "                    raise e\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = options.get_training_parser()\n",
        "    args = options.parse_args_and_arch(parser)\n",
        "\n",
        "    if args.distributed_port > 0 or args.distributed_init_method is not None:\n",
        "        from distributed_train import main as distributed_main\n",
        "\n",
        "        distributed_main(args)\n",
        "    elif args.distributed_world_size > 1:\n",
        "        from multiprocessing_train import main as multiprocessing_main\n",
        "\n",
        "        multiprocessing_main(args)\n",
        "    else:\n",
        "        main(args)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBFgWLQl86bZ",
        "colab_type": "text"
      },
      "source": [
        "# Eval funniness"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0gvbdna8_g8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile eval_scoring_func.py\n",
        "\"\"\"Compute the correlation of funniness scorers given by a scoring function and those\n",
        "given by humans.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "from scipy.stats import spearmanr, zscore\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.feature_selection import f_regression\n",
        "\n",
        "from fairseq.data.dictionary import Dictionary\n",
        "\n",
        "from pungen.scorer import LMScorer, SurprisalScorer, UnigramModel, GoodmanScorer\n",
        "from pungen.options import add_scorer_args, add_generic_args\n",
        "from pungen.utils import logging_config, get_spacy_nlp\n",
        "from pungen.wordvec.generate import SkipGram\n",
        "from pungen.pretrained_wordvec import Glove\n",
        "\n",
        "import logging\n",
        "logger = logging.getLogger('pungen')\n",
        "\n",
        "nlp = get_spacy_nlp(tokenizer='default', disable=['tagger', 'ner', 'parser'])\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    add_scorer_args(parser)\n",
        "    add_generic_args(parser)\n",
        "    parser.add_argument('--skipgram-model', nargs=2, help='pretrained skipgram model [vocab, model]')\n",
        "    parser.add_argument('--skipgram-embed-size', type=int, default=300, help='word embedding size in skipgram model')\n",
        "    parser.add_argument('--human-eval', help='path to human score file')\n",
        "    parser.add_argument('--features', nargs='+', default=['ratio', 'grammar', 'ambiguity'], help='features to analyze')\n",
        "    parser.add_argument('--ignore-cache', action='store_true', help='ignore cached scores. cache path: `args.outdir`/scores.json')\n",
        "    parser.add_argument('--analysis', action='store_true', help='using analysis data instead of generated data')\n",
        "    parser.add_argument('--tokenized', action='store_true', help='whether the sentences are tokenized')\n",
        "    args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "\n",
        "def linear_regression(scores, features, target='human'):\n",
        "    X = np.array([[s[f] for f in features] for s in scores])\n",
        "    y = np.array([s[target] for s in scores])\n",
        "    model = LinearRegression().fit(X, y)\n",
        "    coeffs = {k: v for k, v in zip(features, model.coef_)}\n",
        "    r2 = model.score(X, y)\n",
        "    f_scores, p_values = f_regression(X, y)\n",
        "    f_scores = {k: {'coeff': c, 'fscore': f, 'pvalue': p}\n",
        "            for k, c, f, p in zip(features, model.coef_, f_scores, p_values)}\n",
        "    return model, r2, f_scores\n",
        "\n",
        "def plot(scores, x_features, y_features, path='./', file_format='jpg'):\n",
        "    plt.figure(1)\n",
        "    ind = 0\n",
        "    nrows = len(x_features)\n",
        "    ncols = len(y_features)\n",
        "    f, axarr = plt.subplots(nrows, ncols)\n",
        "    for i, x_feature in enumerate(x_features):\n",
        "        for j, y_feature in enumerate(y_features):\n",
        "            x = np.array([s[x_feature] for s in scores])\n",
        "            y = np.array([s[y_feature] for s in scores])\n",
        "            axarr[ind].scatter(x, y)\n",
        "            axarr[ind].set_xlabel(x_feature)\n",
        "            axarr[ind].set_ylabel(y_feature)\n",
        "            ind += 1\n",
        "    f.subplots_adjust(hspace=0.8)\n",
        "    plt.savefig('x_human.jpg', format=file_format)\n",
        "\n",
        "def parse_human_eval_data(path, tokenized):\n",
        "    candidates = []\n",
        "    with open(path) as fin:\n",
        "        for line in fin:\n",
        "            ss = line.strip().split('\\t')\n",
        "            if tokenized:\n",
        "                sent = ss[0].split()\n",
        "            else:\n",
        "                sent = [x.text for x in nlp(ss[0])]\n",
        "            sent = [x.lower() for x in sent]\n",
        "            pun_word, alter_word = ss[1].split('-')\n",
        "            method = ss[2]\n",
        "            score = float(ss[3])\n",
        "            # TODO: move to preprocess\n",
        "            try:\n",
        "                pun_word_id = sent.index(pun_word)\n",
        "            except ValueError:\n",
        "                continue\n",
        "            c = {\n",
        "                    'pun_sent': sent,\n",
        "                    'pun_word_id': pun_word_id,\n",
        "                    'alter_word': alter_word,\n",
        "                    'scores': {'human': score},\n",
        "                    'type': method,\n",
        "                }\n",
        "            candidates.append(c)\n",
        "    return candidates\n",
        "\n",
        "def score_examples(args):\n",
        "    lm = LMScorer.load_model(args.lm_path)\n",
        "    unigram_model = UnigramModel(args.word_counts_path, args.oov_prob)\n",
        "    skipgram = SkipGram.load_model(args.skipgram_model[0], args.skipgram_model[1], embedding_size=args.skipgram_embed_size, cpu=args.cpu)\n",
        "\n",
        "    candidates = parse_human_eval_data(args.human_eval, args.tokenized)\n",
        "\n",
        "    #vocab = build_vocab(candidates)\n",
        "    #glove = Glove.from_file('/u/scr/nlp/data/glove_vecs/glove.840B.300d.txt', vocab)\n",
        "    glove = None\n",
        "\n",
        "    scorers = [SurprisalScorer(lm, unigram_model, local_window_size=args.local_window_size),\n",
        "               GoodmanScorer(unigram_model, skipgram, glove)]\n",
        "    #scorers = [GoodmanScorer(unigram_model, skipgram, glove)]\n",
        "\n",
        "    for c in candidates:\n",
        "        for scorer in scorers:\n",
        "            scores = scorer.analyze(c['pun_sent'], c['pun_word_id'], c['alter_word'])\n",
        "            c['scores'].update(scores)\n",
        "    return candidates\n",
        "\n",
        "def build_vocab(candidates):\n",
        "    d = Dictionary()\n",
        "    for c in candidates:\n",
        "        for w in c['pun_sent']:\n",
        "            d.add_symbol(w)\n",
        "    return d\n",
        "\n",
        "def compute_stats(candidates):\n",
        "    results_by_type = defaultdict(lambda : defaultdict(float))\n",
        "    for c in candidates:\n",
        "        results_by_type[c['type']]['count'] += 1\n",
        "        results_by_type[c['type']]['score'] += c['scores']['human']\n",
        "    logger.info('Stats')\n",
        "    str_format = '{type:<10s}{count:10d}{score:10.2f}'\n",
        "    for k, v in results_by_type.items():\n",
        "        logger.info(str_format.format(type=k, count=int(v['count']), score=v['score'] / v['count']))\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    json.dump(vars(args), open(os.path.join(args.outdir, 'config.json'), 'w'))\n",
        "\n",
        "    filename = os.path.join(args.outdir, 'scores.json')\n",
        "    if not os.path.exists(filename) or args.ignore_cache:\n",
        "        candidates = score_examples(args)\n",
        "        json.dump(candidates, open(filename, 'w'))\n",
        "    else:\n",
        "        candidates = json.load(open(filename))\n",
        "\n",
        "    candidates = [c for c in candidates if c['type'] in ('pun', 'depun', 'retrieved_pw', 'retrieved_aw', 'nonpun')]\n",
        "\n",
        "    compute_stats(candidates)\n",
        "\n",
        "    # Normalization, cutoff 2-\\sigma\n",
        "    features = candidates[0]['scores'].keys()\n",
        "    for feat in features:\n",
        "        if feat != 'human':\n",
        "            _scores = zscore([c['scores'][feat] for c in candidates])\n",
        "            for _score, c in zip(_scores, candidates):\n",
        "                if _score > 0:\n",
        "                    _score = min(2, _score)\n",
        "                elif _score < 0:\n",
        "                    _score = max(-2, _score)\n",
        "                c['scores'][feat] = _score\n",
        "\n",
        "    # Correlation\n",
        "    if args.analysis:\n",
        "        all_types = tuple(set([c['type'] for c in candidates]))\n",
        "        for types in [('pun',), ('pun', 'depun'), ('pun', 'nonpun', 'retrieved_pw', 'retrieved_aw')]:\n",
        "            _candidates = [c for c in candidates if c['type'] in types]\n",
        "            human_scores = [c['scores']['human'] for c in _candidates]\n",
        "            logger.info('correlation for {} sentences of types {}'.format(len(_candidates), str(types)))\n",
        "            for metric in _candidates[0]['scores'].keys():\n",
        "                if metric == 'human':\n",
        "                    continue\n",
        "                _scores = [c['scores'][metric] for c in _candidates]\n",
        "                corr = spearmanr(human_scores, _scores)\n",
        "                logger.info('{:<15s}: {:>8.4f} p={:>8.4f}'.format(metric, corr.correlation, corr.pvalue))\n",
        "\n",
        "    # All scores\n",
        "    scores = [c['scores'] for c in candidates]\n",
        "\n",
        "    # Plot\n",
        "    #plot(scores, x_features=args.features, y_features=['human'])\n",
        "\n",
        "    # Linear regression\n",
        "    features = args.features\n",
        "    scores = [c['scores'] for c in candidates]\n",
        "    model, r2, feature_stats = linear_regression(scores, features)\n",
        "\n",
        "    logger.info('linear regression')\n",
        "    for name, stats in feature_stats.items():\n",
        "        stats_format = '{name:<15s}' + ' '.join(['{}={{{}:>8.4f}}'.format(s, s) for s in stats.keys()])\n",
        "        logger.info(stats_format.format(name=name, **stats))\n",
        "    logger.info('R^2={:.4f}'.format(r2))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = parse_args()\n",
        "    logging_config(os.path.join(args.outdir, 'console.log'), console_level=logging.DEBUG)\n",
        "    main(args)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9NFKAuX9S3_",
        "colab_type": "text"
      },
      "source": [
        "# Generate puns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5Bln_nm9Vva",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile generate_pun.py \n",
        "import os\n",
        "import pickle\n",
        "import argparse\n",
        "import json\n",
        "from collections import defaultdict\n",
        "import fuzzy\n",
        "\n",
        "from fairseq import options\n",
        "\n",
        "from pungen.retriever import Retriever\n",
        "from pungen.generator import SkipGram, RulebasedGenerator, NeuralCombinerGenerator, RetrieveGenerator, RetrieveSwapGenerator, KeywordsGenerator\n",
        "from pungen.scorer import LMScorer, SurprisalScorer, UnigramModel, RandomScorer, GoodmanScorer\n",
        "from pungen.type import TypeRecognizer\n",
        "from pungen.options import add_scorer_args, add_editor_args, add_retriever_args, add_generic_args, add_type_checker_args\n",
        "from pungen.utils import logging_config, get_lemma, ensure_exist, get_spacy_nlp\n",
        "\n",
        "import logging\n",
        "logger = logging.getLogger('pungen')\n",
        "\n",
        "nlp = get_spacy_nlp()\n",
        "\n",
        "def parse_args():\n",
        "    parser = options.get_generation_parser(interactive=True)\n",
        "    add_scorer_args(parser)\n",
        "    add_editor_args(parser)\n",
        "    add_retriever_args(parser)\n",
        "    add_type_checker_args(parser)\n",
        "    add_generic_args(parser)\n",
        "    parser.add_argument('--pun-words')\n",
        "    parser.add_argument('--system', default='rule')\n",
        "    parser.add_argument('--max-num-examples', type=int, default=-1)\n",
        "    args = options.parse_args_and_arch(parser)\n",
        "    return args\n",
        "\n",
        "def iter_keywords(file_):\n",
        "    with open(file_, 'r') as fin:\n",
        "        for line in fin:\n",
        "            alter_word, pun_word = line.strip().split()\n",
        "            yield alter_word, pun_word\n",
        "\n",
        "def feasible_pun_words(pun_word, alter_word, unigram_model, skipgram=None, freq_threshold=1000):\n",
        "    # Pun / alternative word cannot be phrases\n",
        "    if len(alter_word.split('_')) > 1 or len(pun_word.split('_')) > 1:\n",
        "        logger.info('FAIL: phrase')\n",
        "        return False, 'phrase'\n",
        "\n",
        "    if skipgram and skipgram.vocab.index(get_lemma(pun_word)) == skipgram.vocab.unk():\n",
        "        logger.info('FAIL: unknown pun word: {}'.format(pun_word))\n",
        "        return False, 'unk to skipgram'\n",
        "\n",
        "    return True, None\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    ensure_exist(args.outdir, is_dir=True)\n",
        "    json.dump(vars(args), open(os.path.join(args.outdir, 'config.json'), 'w'))\n",
        "\n",
        "    unigram_model = UnigramModel(args.word_counts_path, args.oov_prob)\n",
        "    retriever = Retriever(args.doc_file, path=args.retriever_model, overwrite=args.overwrite_retriever_model)\n",
        "\n",
        "    if args.system.startswith('rule') or args.system == 'keywords' or args.scorer in ('goodman',):\n",
        "        skipgram = SkipGram.load_model(args.skipgram_model[0], args.skipgram_model[1], embedding_size=args.skipgram_embed_size, cpu=args.cpu)\n",
        "    else:\n",
        "        skipgram = None\n",
        "\n",
        "    if args.scorer == 'random':\n",
        "        scorer = RandomScorer()\n",
        "    elif args.scorer == 'surprisal':\n",
        "        lm = LMScorer.load_model(args.lm_path)\n",
        "        scorer = SurprisalScorer(lm, unigram_model, local_window_size=args.local_window_size)\n",
        "    elif args.scorer == 'goodman':\n",
        "        scorer = GoodmanScorer(unigram_model, skipgram)\n",
        "\n",
        "    type_recognizer = TypeRecognizer(threshold=args.type_consistency_threshold)\n",
        "\n",
        "    if args.system == 'rule':\n",
        "        generator = RulebasedGenerator(retriever, skipgram, type_recognizer, scorer, dist_to_pun=args.distance_to_pun_word)\n",
        "    elif args.system == 'rule+neural':\n",
        "        generator = NeuralCombinerGenerator(retriever, skipgram, type_recognizer, scorer, args.distance_to_pun_word, args)\n",
        "    elif args.system == 'retrieve':\n",
        "        generator = RetrieveGenerator(retriever, scorer)\n",
        "    elif args.system == 'retrieve+swap':\n",
        "        generator = RetrieveSwapGenerator(retriever, scorer)\n",
        "\n",
        "    puns = json.load(open(args.pun_words))\n",
        "    # Uniq\n",
        "    d = {}\n",
        "    for e in puns:\n",
        "        d[e['pun_word']] = e\n",
        "    puns = d.values()\n",
        "    # Sorting by quality of pun words\n",
        "    dmeta = fuzzy.DMetaphone()\n",
        "    homophone = lambda x, y: float(dmeta(x)[0] == dmeta(y)[0])\n",
        "    length = lambda x, y: float(len(x) > 2 and len(y) > 2)\n",
        "    freq = lambda x, y: unigram_model.word_counts.get(x, 0) * unigram_model.word_counts.get(y, 0)\n",
        "    puns = sorted(puns, key=lambda e: (length(e['pun_word'], e['alter_word']),\n",
        "                                       homophone(e['pun_word'], e['alter_word']),\n",
        "                                       freq(e['pun_word'], e['alter_word'])),\n",
        "                  reverse=True)\n",
        "    num_success = 0\n",
        "    processed_examples = []\n",
        "    for example in puns:\n",
        "        pun_word, alter_word = example['pun_word'], example['alter_word']\n",
        "        logger.info('-'*50)\n",
        "        logger.info('INPUT: alter={} pun={}'.format(alter_word, pun_word))\n",
        "        logger.info('REFERENCE: {}'.format(' '.join(example['tokens'])))\n",
        "        logger.info('-'*50)\n",
        "\n",
        "        feasible, reason = feasible_pun_words(pun_word, alter_word, unigram_model, skipgram=skipgram, freq_threshold=args.pun_freq_threshold)\n",
        "        if not feasible:\n",
        "            example['fail'] = reason\n",
        "            continue\n",
        "\n",
        "        results = generator.generate(alter_word, pun_word, k=args.num_topic_words, ncands=args.num_candidates, ntemps=args.num_templates)\n",
        "        example['results'] = results\n",
        "        if not results:\n",
        "            continue\n",
        "\n",
        "        results = [r for r in results if r.get('score') is not None]\n",
        "        results = sorted(results, key=lambda r: r['score'], reverse=True)\n",
        "        for r in results[:3]:\n",
        "            logger.info('{:<8.2f}{}'.format(r['score'], ' '.join(r['output'])))\n",
        "\n",
        "        processed_examples.append(example)\n",
        "        num_success += 1\n",
        "        if args.max_num_examples > 0 and num_success >= args.max_num_examples:\n",
        "            break\n",
        "\n",
        "    json.dump(processed_examples, open(os.path.join(args.outdir, 'results.json'), 'w'))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = parse_args()\n",
        "    logging_config(os.path.join(args.outdir, 'generate_pun.log'), console_level=logging.INFO)\n",
        "    main(args)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}